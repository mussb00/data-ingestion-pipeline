{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mussb00/data-ingestion-pipeline/blob/main/data_ingestion_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wut-bzjncR2F"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q Kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!echo '{\"username\":\"mussieberhane\",\"key\":\"f3f6c8621a8237750399d4f577288380\"}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c wikichallenge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsY9GkT1iV2X"
      },
      "outputs": [],
      "source": [
        "!unzip /content/wikichallenge.zip -d /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5CVItSNGkCb"
      },
      "outputs": [],
      "source": [
        "!pip install py7zr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBv_20u0LrNU"
      },
      "outputs": [],
      "source": [
        "import py7zr\n",
        "with py7zr.SevenZipFile('/content/wikichallenge_data_all.7z', mode='r') as z:\n",
        "    z.extractall('/content/large_file')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETRUGOPqMU_W"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "sz=Path('/content/large_file/comments.tsv').stat().st_size\n",
        "print(sz) # size in bytes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> __Pandas will struggle to read large datasets__\n",
        "\n"
      ],
      "metadata": {
        "id": "FPO_Rtsotw0n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc6_SfQ5TYj7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# reading without using chunks\n",
        "\n",
        "s_time=time.time()\n",
        "df= pd.read_csv('/content/hiiii/comments.tsv', sep='\\t')\n",
        "e_time=time.time()\n",
        "\n",
        "print(\"Read without chunks: \", (e_time-s_time), \"seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chunking is the process of reading a large dataset in smaller, more manageablechunks and processing these chunks. This means the memory requirements to read a dataset are lower. Thus, it improves the speed of reading data since by not exceeding the physical memory limits, the computer then does not have to access data using virtual memory (disk storage) which is slower.\n"
      ],
      "metadata": {
        "id": "mNfHe56vt-Q7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8nFapfcbGYC"
      },
      "outputs": [],
      "source": [
        "# reading with chunks\n",
        "s_time_chunk=time.time()\n",
        "chunk= pd.read_csv('/content/large_file/comments.tsv', sep='\\t', chunksize=1000)\n",
        "e_time_chunk=time.time()\n",
        "\n",
        "print(\"Read with chunks: \", (e_time_chunk-s_time_chunk), \"seconds\")\n",
        "df=pd.concat(chunk)\n",
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dask is a parallel-computing library. This means it distributes processing tasks across mutliple CPU cores on a single host machine and then combines the results. This leads to massive improvements in performance on computational tasks for large datasets."
      ],
      "metadata": {
        "id": "6NtT4AN7ue1q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOFt5s6sT712"
      },
      "outputs": [],
      "source": [
        "!pip install dask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbnxnQ0xURU5"
      },
      "outputs": [],
      "source": [
        "from dask import dataframe as df1\n",
        "\n",
        "# read with dask\n",
        "s_time_dask=time.time()\n",
        "dask_df=df1.read_csv('/content/large_file/comments.tsv', sep='\\t')\n",
        "e_time_dask=time.time()\n",
        "\n",
        "print(\"Read with dask: \", (e_time_dask-s_time_dask), \"seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_8Cmb03Ue4o"
      },
      "outputs": [],
      "source": [
        "dask_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dask_df['comment'].dtype"
      ],
      "metadata": {
        "id": "2WackqW71YBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvKYsQtIUqbT"
      },
      "outputs": [],
      "source": [
        "## https://www.geeksforgeeks.org/working-with-large-csv-files-in-python/ \n",
        "## https://pythonspeed.com/articles/faster-pandas-dask/#:~:text=When%20data%20doesn't%20fit,can%20also%20become%20a%20bottleneck.\n",
        "## this article explains how exactly dask works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWXN5gM-fTmH"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHeEp5TWflCA"
      },
      "outputs": [],
      "source": [
        "%%writefile testutility.py \n",
        "import logging\n",
        "import os\n",
        "import yaml\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import re\n",
        "\n",
        "def read_config_file(filepath):\n",
        "  with open(filepath, 'r') as stream:\n",
        "    try:\n",
        "      return yaml.safe_load(stream)\n",
        "    except yaml.YAMLError as exc:\n",
        "      logging.error(exc)\n",
        "\n",
        "def replacer(string, char):\n",
        "  pattern=char+'{2,}'\n",
        "  string=re.sub(pattern, char, string)\n",
        "  return string\n",
        "\n",
        "def col_header_val(df,table_config):\n",
        "    '''\n",
        "    replace whitespaces in the column\n",
        "    and standardized column names\n",
        "    '''\n",
        "    df.columns = df.columns.str.lower()\n",
        "    df.columns = df.columns.str.replace('[^\\w]','_',regex=True)\n",
        "    df.columns = list(map(lambda x: x.strip('_'), list(df.columns)))\n",
        "    df.columns = list(map(lambda x: replacer(x,'_'), list(df.columns)))\n",
        "    expected_col = list(map(lambda x: x.lower(),  table_config['columns']))\n",
        "    expected_col.sort()\n",
        "    df.columns =list(map(lambda x: x.lower(), list(df.columns)))\n",
        "    df = df.reindex(sorted(df.columns), axis=1)\n",
        "    if len(df.columns) == len(expected_col) and list(expected_col)  == list(df.columns):\n",
        "        print(\"column name and column length validation passed\")\n",
        "        return 1\n",
        "    else:\n",
        "        print(\"column name and column length validation failed\")\n",
        "        mismatched_columns_file = list(set(df.columns).difference(expected_col))\n",
        "        print(\"Following File columns are not in the YAML file\",mismatched_columns_file)\n",
        "        missing_YAML_file = list(set(expected_col).difference(df.columns))\n",
        "        print(\"Following YAML columns are not in the file uploaded\",missing_YAML_file)\n",
        "        logging.info(f'df columns: {df.columns}')\n",
        "        logging.info(f'expected columns: {expected_col}')\n",
        "        return 0\n",
        "\n",
        "def col_number_val(df, table_config):\n",
        "    '''\n",
        "    Check the number of columns coincides coincides with YAML file\n",
        "    '''\n",
        "    if len(df.columns)==len(table_config['columns']):\n",
        "        print('column length validation passed')\n",
        "        return 1\n",
        "\n",
        "def column_data_type_validator(df, table_config):\n",
        "    '''\n",
        "    Check data is in the right format\n",
        "    '''\n",
        "    if df['revision_id'].dtype == table_config['datatype']['revision_id'] and df['comments'].dtype==table_config['datatype']['comments']:\n",
        "        print('datatype matches configuration')\n",
        "        return 0\n",
        "    else:\n",
        "      print('at least one of the column data types are incorrect')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRvzOhDLjbyw"
      },
      "outputs": [],
      "source": [
        "!pip install pyyaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6TWBfqlaMPL"
      },
      "outputs": [],
      "source": [
        "## Write YAML file\n",
        "\n",
        "%%writefile file.yaml\n",
        "file_type: tsv\n",
        "file_name: comments\n",
        "inbound_delimeter: \"\\t\"\n",
        "outbound_delimeter: \"|\"\n",
        "columns:\n",
        "  - revision_id\n",
        "  - comments\n",
        "datatype:\n",
        "  revision_id: int64 # integer\n",
        "  comments: O # string/pandas object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuaYBQ6tfK7e"
      },
      "outputs": [],
      "source": [
        "import testutility as util\n",
        "\n",
        "config_data=util.read_config_file(\"file.yaml\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e7vfAUGm-Yu"
      },
      "outputs": [],
      "source": [
        "util.col_header_val(dask_df, config_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# write data to pipe separated text file in .gz format using pandas\n",
        "\n",
        "df.to_csv('textfile.txt', header=None, index=None, sep='|', mode='w', compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1})"
      ],
      "metadata": {
        "id": "iQtqdFVy4rqZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMY/x9b00mKRLThSq/1Wrwg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}